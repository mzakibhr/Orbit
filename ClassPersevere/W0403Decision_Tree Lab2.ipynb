{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrJMbfkVyX1q"
   },
   "source": [
    "#  Module 5: Supervised Machine Learning and Predictive Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD8gRrBsyZBa"
   },
   "source": [
    "## Lab 2: Hyperparameter tuning for Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYaJQkcTyd8s"
   },
   "source": [
    "## Objective\n",
    "***\n",
    "\n",
    "* Hyperparameter tuning in Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyVEoE_gze53"
   },
   "source": [
    "### Hyperparameter tuning in Decision Tree \n",
    "***\n",
    "Decision tree is a widely-used supervised learning algorithm which is suitable for both classification and regression tasks. Decision trees serve as building blocks for some prominent ensemble learning algorithms such as random forests, GBDT, and XGBOOST.  \n",
    "\n",
    "On the downside, decision trees are prone to overfitting. They can easily become over-complex which prevents them from generalizing well to the structure in the dataset. In that case, the model is likely to end up overfitting which is a serious issue in machine learning. \n",
    "\n",
    "To overcome this issue, we need to carefully adjust the hyperparameters of decision trees. \n",
    "\n",
    "In this exercise, we will learn tree visualization with hyperparameters tuning. \n",
    "\n",
    "We will use one of the built-in datasets of scikit-learn. The wine dataset contains 13 features (i.e., Columns) on three different wine classes. There are 178 samples (i.e., rows) in the dataset. \n",
    "\n",
    "We import the libraries and the load_wine dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHehntAKVsSE"
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Loading wine dataset\n",
    "\n",
    "x, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lI2v6ka4ug-m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImJwyklv00JA"
   },
   "source": [
    "Now we fit the DecisionTreeClassifier model on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya00VnroWiG9"
   },
   "outputs": [],
   "source": [
    "# Train a decision tree classifier on wine dataset\n",
    "\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAlTuPz3YyBu"
   },
   "outputs": [],
   "source": [
    "# Visualizing decision tree classifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(24,14))\n",
    "tree.plot_tree(clf, filled=True, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew0sGw560pQf"
   },
   "source": [
    "We observe that the model keeps splitting up until all the nodes are pure (I.e., contains sample from only one class). \n",
    "\n",
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. If you take a look the leaf nodes (the nodes at the end of tree), you will see that gini is equal to zero. \n",
    "\n",
    "The other function to evaluate the quality of a split is entropy which is a measure of uncertainty or randomness. The more randomness a variable has, the higher the entropy is. \n",
    "\n",
    "We usually do not want a tree with all pure leaf nodes. It would be too specific and likely to overfit. \n",
    "\n",
    "When the algorithm performs a split, the main goal is to decrease impurity as much as possible. The more the impurity decreases, the more informative power that split gains. As the tree gets deeper, the amount of impurity decrease becomes lower. We can use this to prevent the tree from doing further splits. \n",
    "\n",
    "The hyperparameter for this task is **min_impurity_decrease**. It is set to zero by default. Letâ€™s change it and see the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRMjij1CY11E"
   },
   "outputs": [],
   "source": [
    "# Customizing min_impurity_decrease\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.02)\n",
    "clf.fit(x, y)\n",
    "plt.figure(figsize=(18,10))\n",
    "tree.plot_tree(clf, filled=True, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7glKE5qK1Q-o"
   },
   "source": [
    "We now have a much smaller tree. Consider the green node at the bottom. It contains 65 samples and 61 of them belong to one class. There is no need to further split that node because we can afford to have 4 misclassified samples out of 65. If we keep splitting that node, the model will probably be overfitting.  \n",
    "\n",
    "Min_impurity_split parameter can be used to control the tree based on impurity values. It sets a threshold on gini. For instance, if **min_impurity_split** is set to 0.3, a node needs to have a gini value that is more than 0.3 to be further splitted. \n",
    "\n",
    " \n",
    "\n",
    "Another hyperparameter to control the depth of a tree is max_depth. It does not make any calculations regarding impurity or sample ratio. The model stops splitting when **max_depth** is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZUmGt2fbOi7"
   },
   "outputs": [],
   "source": [
    "# Tuning max_depth\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(x, y)\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(clf, filled=True, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izhofTBl11rk"
   },
   "source": [
    "**Max_depth** is less flexible compared to **min_impurity_decrease**. This actually brings us to another hyperparameter which is **min_samples_leaf**. It indicates the minimum number of samples required to be at a leaf node. We need to be careful when using hyperparameters together. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCg7hpVwsIQ-"
   },
   "outputs": [],
   "source": [
    "# Tuning max_depth and min_samples_leaf\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=3)\n",
    "clf.fit(x, y)\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(clf, filled=True, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxHSJkDM1-p9"
   },
   "source": [
    "In this case, min_samples_leaf is actually harmful for the model. It did not prevent the model from doing that final split. \n",
    "\n",
    "We can also limit the number of leaf nodes using **max_leaf_nodes** parameter which grows the tree in best-first fashion until max_leaf_nodes reached. The best split is decided based on impurity decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFho2lqds5IS"
   },
   "outputs": [],
   "source": [
    "# Tuning max_leaf_nodes\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=5)\n",
    "clf.fit(x, y)\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(clf, filled=True, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5g6zIKt2EZZ"
   },
   "source": [
    "\n",
    "We end up having a tree with 5 leaf nodes. \n",
    "\n",
    "Another important hyperparameter of decision trees is max_features which is the number of features to consider when looking for the best split.  \n",
    "\n",
    "If not specified, the model considers all of the features. There are 13 features in our dataset. If we set max_features as 5, the model randomly selects 5 features to decide on the next split. Max_features parameter also helps preventing the model from overfitting but it is not enough just to use max_features.  \n",
    "\n",
    "If we let the model to become too deep, it will end up using all the features anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_fgNT1UyiXm"
   },
   "source": [
    "## Thank You !"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Module5_Section1_Lab2_Decision_Tree.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
